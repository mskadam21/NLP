{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AV - Sentiment Analysis - NLP Project\n",
    "\n",
    "Problem Statement\n",
    "Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products.\n",
    "\n",
    " \n",
    "\n",
    "Evaluation Metric\n",
    "The metric used for evaluating the performance of classification model would be weighted F1-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    " **Import the usual suspects. :) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "**Read the train.csv and test.csv file **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check the head, info , and describe methods on yelp.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Let's explore the data\n",
    "\n",
    "## Imports\n",
    "\n",
    "**Import the data visualization libraries if you haven't done so already.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use FacetGrid from the seaborn library to create a grid of 5 histograms of text length based off of the star ratings. Reference the seaborn documentation for hints on this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet_len'] = train['tweet'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train,col='label')\n",
    "g.map(plt.hist,'tweet_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a boxplot of text length for each star category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='label',y='tweet_len',data=train,palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a countplot of the number of occurrences for each type of star rating.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label',data=train,palette='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create two objects X and y. X will be the 'text' column of yelp_class and y will be the 'stars' column of yelp_class. (Your features and target/labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['tweet']\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    word_list = [word.lower() for word in string.split()]\n",
    "    stopwords_list = list(stopwords.words(\"english\"))\n",
    "    for word in word_list:\n",
    "        if word in stopwords_list:\n",
    "            word_list.remove(word)\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fingerprint pregnancy test goo gl h wouldmfqv ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>finally transparant silicon case thanks my unc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>love would go talk makememories unplug relax i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>wired know george made way iphone cute daventr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>amazing service apple even talk me question un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>iphone software update fucked my phone big tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>happy us instapic instadaily us sony xperia xp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>new type charger cable uk www ebay co uk itm w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>bout go shopping listening music iphone justme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>photo fun selfie pool water sony camera picoft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0  fingerprint pregnancy test goo gl h wouldmfqv ...\n",
       "1   2      0  finally transparant silicon case thanks my unc...\n",
       "2   3      0  love would go talk makememories unplug relax i...\n",
       "3   4      0  wired know george made way iphone cute daventr...\n",
       "4   5      1  amazing service apple even talk me question un...\n",
       "5   6      1  iphone software update fucked my phone big tim...\n",
       "6   7      0  happy us instapic instadaily us sony xperia xp...\n",
       "7   8      0  new type charger cable uk www ebay co uk itm w...\n",
       "8   9      0  bout go shopping listening music iphone justme...\n",
       "9  10      0  photo fun selfie pool water sony camera picoft..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'] = train['tweet'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\W',' ',str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n",
    "train['tweet'] = train['tweet'].str.lower()\n",
    "\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\â€™\", \"\\'\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"n\\â€™t\", \" not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\â€™d\", \" would\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n",
    "train['tweet'] = train['tweet'].map(lambda x: re.sub(r'[.|,|)|(|\\|/]',r' ', str(x)))\n",
    "train['tweet'] = train['tweet'].apply(lambda x: remove_stopwords(x))\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>9864</td>\n",
       "      <td>men the top gummers mommyhood iphone life inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>9865</td>\n",
       "      <td>am thoroughly addicted the angrybirds game con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>9866</td>\n",
       "      <td>girl brazil life galaxys would samsung saturda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>9867</td>\n",
       "      <td>whoop raj bigbangtheory not alone secret siri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>9868</td>\n",
       "      <td>you camera everytime look you smile boredom ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>9869</td>\n",
       "      <td>samsunggalaxynote would explodes burns would y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>9870</td>\n",
       "      <td>available hoodie check out http zetasupplies c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>9871</td>\n",
       "      <td>goes crack right across screen you could actua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>9872</td>\n",
       "      <td>codeofinterest said adobe big time may well in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>9873</td>\n",
       "      <td>finally got thanx father samsung galaxy would ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "1943  9864  men the top gummers mommyhood iphone life inst...\n",
       "1944  9865  am thoroughly addicted the angrybirds game con...\n",
       "1945  9866  girl brazil life galaxys would samsung saturda...\n",
       "1946  9867  whoop raj bigbangtheory not alone secret siri ...\n",
       "1947  9868  you camera everytime look you smile boredom ca...\n",
       "1948  9869  samsunggalaxynote would explodes burns would y...\n",
       "1949  9870  available hoodie check out http zetasupplies c...\n",
       "1950  9871  goes crack right across screen you could actua...\n",
       "1951  9872  codeofinterest said adobe big time may well in...\n",
       "1952  9873  finally got thanx father samsung galaxy would ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['tweet'] = test['tweet'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\W',' ',str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n",
    "test['tweet'] = test['tweet'].str.lower()\n",
    "\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\â€™\", \"\\'\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"n\\â€™t\", \" not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\â€™d\", \" would\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n",
    "test['tweet'] = test['tweet'].map(lambda x: re.sub(r'[.|,|)|(|\\|/]',r' ', str(x)))\n",
    "test['tweet'] = test['tweet'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try lemmatization  1) using nltk.stem wordnet \n",
    "# Can word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "def get_normalize(string):\n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # 2. Lemmatize Single Word with the appropriate POS tag\n",
    "    #word = 'feet'\n",
    "    #print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "    # 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "    #sentence = \"The striped bats are hanging on their feet for best\"\n",
    "    #print('Example:')\n",
    "    #print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "    #> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n",
    "    new_list = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(string)]\n",
    "    return ' '.join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fingerprint pregnancy test goo gl h wouldmfqv ...\n",
       "1    finally transparant silicon case thanks my unc...\n",
       "2    love would go talk makememories unplug relax i...\n",
       "3    wired know george made way iphone cute daventr...\n",
       "4    amazing service apple even talk me question un...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazing service apple even talk me question unless pay would would would would their stupid support'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amaze service apple even talk me question unless pay would would would would their stupid support'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_normalize(train['tweet'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet1'] = train['tweet'].apply(lambda x: get_normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920, 11084)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}', ngram_range=(1, 4), use_idf=1,smooth_idf=1,\n",
    "                      sublinear_tf=1, stop_words = 'english')\n",
    "\n",
    "train_tfidf = vec.fit_transform(train['tweet1'])\n",
    "\n",
    "# TF-IDF\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1953, 11084)\n"
     ]
    }
   ],
   "source": [
    "test_tfidf = vec.transform(test['tweet'])\n",
    "print(test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(min_df=3, max_features=None, strip_accents='unicode', analyzer='word',\n",
    "                      token_pattern=r'\\w{1,}', ngram_range=(1, 4), use_idf=1,smooth_idf=1,\n",
    "                      sublinear_tf=1, stop_words = 'english')\n",
    "\n",
    "X_train_tfidf = vec.fit_transform(train['tweet'])\n",
    "\n",
    "# TF-IDF\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vec.transform(test['tweet'])\n",
    "\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(C=2.5,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkadam/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=101, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier.fit(train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_classifier.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  label\n",
       "0  7921      0\n",
       "1  7922      0\n",
       "2  7923      0\n",
       "3  7924      0\n",
       "4  7925      0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['label'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  label\n",
       "0  7921      1\n",
       "1  7922      0\n",
       "2  7923      1\n",
       "3  7924      1\n",
       "4  7925      1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('Submission009.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [10, 50, 100, 1000, 10000]:\n",
    "    lr = LogisticRegression(C=c, random_state=101).fit(X_train_tfidf, y_train)\n",
    "    #lr = LogisticRegression(C=c, random_state=2019).fit(X_train_tfidf, y_train)\n",
    "    print (\"f1 score for C=%s: %s\" % (c, f1_score(y_val, lr.predict(X_val_tfidf))))\n",
    "\n",
    "#Accuracy for C=50: 0.9627717816361645\n",
    "#f1 score for C=1000: 0.999298245614035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=2.5,random_state=2019).fit(X_train_tfidf, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              default: 'liblinear'.\n",
    "\n",
    "    Algorithm to use in the optimization problem.\n",
    "\n",
    "    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
    "      'saga' are faster for large ones.\n",
    "    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
    "      handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
    "      schemes.\n",
    "    - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
    "      'liblinear' and 'saga' handle L1 penalty. \n",
    "      \n",
    "      #   penalty='l2',\n",
    "    dual=True,\n",
    "    tol=0.0001,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    class_weight=None, #dict or 'balanced', default: None\n",
    "    solver='warn', #solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, def= 'warn'\n",
    "    max_iter=100,\n",
    "    multi_class='warn',\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = log_reg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3 = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3['label'] = pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3.to_csv('Submission003.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes  import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = BernoulliNB().fit(train_tfidf,train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred5 = cls.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3 = pd.read_csv('sample_submission.csv')\n",
    "sub3['label'] = pred5\n",
    "sub3.to_csv('Submission003.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm  import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = SVC(C=1,degree=1,gamma=1,kernel='sigmoid').fit(X_train_tfidf,train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred5 = cls.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3 = pd.read_csv('sample_submission.csv')\n",
    "sub3['label'] = pred5\n",
    "sub3.to_csv('Submission003.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create param grid\n",
    "param_grid = {'C': [1], 'degree': [1], 'gamma': [1], 'kernel': ['sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(coef0=0.5),param_grid,refit=True,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train_tfidf,train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['len'] = test['tweet'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble  import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    " n_estimators= 300,\n",
    " min_samples_split= 10,\n",
    " min_samples_leaf= 4,\n",
    " max_features= 'sqrt',\n",
    " max_depth= 90,\n",
    " bootstrap= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(X_train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['label']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('Submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 18 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 9, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost  import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.fit(tweet_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_classifier.predict(test_tweet_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub4 = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub4['label'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub4.to_csv('Submission4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
